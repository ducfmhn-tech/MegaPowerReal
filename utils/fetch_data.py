import requests
import pandas as pd
from io import StringIO
from datetime import datetime
from utils.logger import log

# -------------------------------------------------------------
# C·∫•u h√¨nh request
# -------------------------------------------------------------
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/123.0 Safari/537.36"
}

TIMEOUT = 30


# -------------------------------------------------------------
# Chu·∫©n h√≥a b·∫£ng v·ªÅ d·∫°ng (draw_date, n1..n6)
# -------------------------------------------------------------
def normalize_dataframe(df, url):
    df.columns = df.columns.map(lambda x: str(x).strip())

    # T·ª± ƒë·ªông t√¨m c·ªôt ng√†y
    date_cols = [c for c in df.columns if "ng√†y" in c.lower() or "date" in c.lower()]
    if not date_cols:
        # M·ªôt s·ªë website kh√¥ng c√≥ header ‚Üí c·ªôt 0 l√† ng√†y
        date_col = df.columns[0]
    else:
        date_col = date_cols[0]

    # L·∫•y 7 c·ªôt ƒë·∫ßu n·∫øu kh√¥ng r√µ
    df = df.iloc[:, :7]
    df = df.copy()
    df.columns = ["draw_date", "n1", "n2", "n3", "n4", "n5", "n6"]

    # Chu·∫©n h√≥a ng√†y
    try:
        df["draw_date"] = df["draw_date"].astype(str)
        df["draw_date"] = df["draw_date"].str.extract(r"(\d{1,2}/\d{1,2}/\d{4})")[0]
        df["draw_date"] = pd.to_datetime(df["draw_date"], format="%d/%m/%Y", errors="coerce")
    except:
        pass

    # √âp s·ªë
    for col in ["n1", "n2", "n3", "n4", "n5", "n6"]:
        df[col] = pd.to_numeric(df[col], errors="coerce").astype("Int64")

    df = df.dropna(subset=["draw_date", "n1", "n2", "n3", "n4", "n5", "n6"])

    log(f"‚úî Chu·∫©n h√≥a th√†nh c√¥ng b·∫£ng t·ª´ {url}: {len(df)} rows")
    return df


# -------------------------------------------------------------
# Parse b·∫£ng HTML ‚Üí DataFrame (kh√¥ng l·ªói MultiIndex)
# -------------------------------------------------------------
def parse_table(html, url):
    try:
        tables = pd.read_html(StringIO(html))
    except Exception as e:
        log(f"‚ùå Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c b·∫£ng HTML t·ª´ {url}: {e}")
        return pd.DataFrame()

    if not tables:
        log(f"‚ö† Kh√¥ng t√¨m th·∫•y b·∫£ng HTML tr√™n {url}")
        return pd.DataFrame()

    # L·∫•y b·∫£ng c√≥ nhi·ªÅu d√≤ng nh·∫•t
    df = max(tables, key=lambda t: len(t))

    if df.empty or len(df.columns) < 7:
        log(f"‚ö† B·∫£ng kh√¥ng h·ª£p l·ªá tr√™n {url}")
        return pd.DataFrame()

    return normalize_dataframe(df, url)


# -------------------------------------------------------------
# Fetch 1 ngu·ªìn
# -------------------------------------------------------------
def fetch_one_source(url, limit=200):
    log(f"üîπ Fetching {url} ...")

    html = None

    # Retry 3 l·∫ßn
    for attempt in range(1, 4):
        try:
            r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
            if r.status_code == 200:
                html = r.text
                break
            else:
                log(f"‚ö† L·ªói HTTP {r.status_code} ({url}) attempt {attempt}")
        except Exception as e:
            log(f"‚ùå L·ªói fetch {url} (attempt {attempt}): {e}")

    if not html:
        log(f"‚ùå B·ªè qua {url} v√¨ kh√¥ng fetch ƒë∆∞·ª£c.")
        return pd.DataFrame()

    df = parse_table(html, url)

    if df.empty:
        log(f"‚ö† Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ {url}")
        return df

    df = df.sort_values("draw_date", ascending=False).head(limit)

    log(f"‚úî Fetched {len(df)} rows from {url}")
    return df


# -------------------------------------------------------------
# Fetch to√†n b·ªô ngu·ªìn Mega / Power
# -------------------------------------------------------------
def fetch_all_sources(urls, limit=200):
    all_rows = []

    REQUIRED = ["draw_date", "n1", "n2", "n3", "n4", "n5", "n6"]

    log(f"==== B·∫ÆT ƒê·∫¶U FETCH {len(urls)} NGU·ªíN ====")

    for url in urls:
        df = fetch_one_source(url, limit)
        if df is None or df.empty:
            continue

        # ƒê·∫£m b·∫£o c·ªôt ƒë√∫ng
        df.columns = df.columns.map(str)

        missing = [c for c in REQUIRED if c not in df.columns]
        if missing:
            log(f"‚ö† B·ªè qua {url} v√¨ thi·∫øu c·ªôt {missing}")
            continue

        # Gi·ªØ ƒë√∫ng c·ªôt
        df = df[REQUIRED]

        all_rows.append(df)

    if not all_rows:
        log("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá t·ª´ b·∫•t k·ª≥ ngu·ªìn n√†o!")
        return pd.DataFrame(columns=REQUIRED)

    out = pd.concat(all_rows, ignore_index=True)

    # Xo√° tr√πng
    try:
        out = out.drop_duplicates(subset=REQUIRED)
    except Exception as e:
        log(f"‚ö† Kh√¥ng th·ªÉ drop duplicates: {e}")

    out = out.sort_values("draw_date", ascending=False).head(limit)

    log(f"üìå Fetch xong: Mega/Power = {len(out)} rows h·ª£p l·ªá")
    return out
